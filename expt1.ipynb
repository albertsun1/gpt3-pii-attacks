{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment 1 code\n",
    "We use two types of prompts in this experiment:\n",
    "\n",
    "1. 100-char length common crawl prompts,\n",
    "2. Empty string generations (idea inspired by the data extraction attack from the [Lukas et. al](https://arxiv.org/pdf/2302.00539.pdf)) paper\n",
    "\n",
    "Much of the testing code was drawn from the open source code released by the [Diera et. al paper](https://arxiv.org/abs/2212.03749). To replicate the below tests, get the finetuned Enron data from the Github repo linked in their paper."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing functions: \n",
    "\n",
    "- `read_text_file` - Read .txt file into string\n",
    "- `test` - input the generations output to test how many PIIs appear from the original fine-tuning dataset\n",
    "- `test_set_difference` - input the generations output from the fine-tuned and base model to do a set difference and just see the unique PIIs that only appeared in the fine-tuned model generations\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_text_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def test_set_difference(text_ft, text_base):\n",
    "    import json\n",
    "    from collections import defaultdict\n",
    "    from collections import Counter\n",
    "    import numpy as np\n",
    "    from ahocorapy.keywordtree import KeywordTree\n",
    "    from timeit import default_timer as timer\n",
    "    ################################################\n",
    "    # Experiment Setup\n",
    "    ################################################\n",
    "\n",
    "    entity_file = \"expt1-data/ner_data_enron_wo_pretrained.json\"\n",
    "    # Modify the variable to your test string\n",
    "    # test_string = concatenated_text\n",
    "    k_value = 1  # parameter for k-eidetic search, None if looking for all entities\n",
    "\n",
    "    ################################################\n",
    "    # Create list of given entity type\n",
    "    ################################################\n",
    "    def process_entity_type(entities, type):\n",
    "        ent_list = []\n",
    "        for k, v in entities.items():\n",
    "            if k == type:\n",
    "                for s in v:\n",
    "                    entity = s.strip()\n",
    "                    if len(entity) > 3:\n",
    "                        ent_list.append(entity)\n",
    "        return ent_list\n",
    "\n",
    "    ################################################\n",
    "    # Find entities in text samples\n",
    "    ################################################\n",
    "    def find_entities(entities, text, type, k_value=None):\n",
    "        kwtree = KeywordTree(case_insensitive=False)\n",
    "        val, cnt = np.unique(entities, return_counts=True)\n",
    "\n",
    "        if k_value is not None:\n",
    "            eidetic_ents = val[cnt == 1]\n",
    "            # print(\"Number of \", k_value, \" eidetic\", type, \" ents: \", eidetic_ents.size)\n",
    "            for ent in eidetic_ents:\n",
    "                kwtree.add(ent)\n",
    "        else:\n",
    "            unique_ents = set(val)\n",
    "            # print(\"Number of unique\", type, \" ents: \", len(list(unique_ents)))\n",
    "            for ent in unique_ents:\n",
    "                kwtree.add(ent)\n",
    "\n",
    "        kwtree.finalize()\n",
    "        lines = text.split('\\n')  # Split the test_string into lines\n",
    "        results = kwtree.search_all(' '.join(lines))\n",
    "        result_set = set([result[0] for result in results])\n",
    "        return result_set\n",
    "    def main():\n",
    "        start = timer()\n",
    "\n",
    "        all_count = 0\n",
    "\n",
    "        all_ent = json.load(open(\"extractionfiles/\" + entity_file))\n",
    "\n",
    "        search_types = [\"PERSON\", \"ORG\", \"LOC\", \"GPE\", \"FAC\", \"MONEY\", \"CARDINAL\"]\n",
    "\n",
    "        for type in search_types:\n",
    "            select_ents = process_entity_type(all_ent, type)\n",
    "            found_ents_1 = find_entities(select_ents, text_ft, type, k_value)\n",
    "            found_ents_2 = find_entities(select_ents, text_base, type, k_value)\n",
    "            set_diff = found_ents_1 - found_ents_2\n",
    "            print(type, \" count: \", len(set_diff))\n",
    "            print(sorted(list(set_diff), key=len, reverse=True))\n",
    "            all_count += len(set_diff)\n",
    "            end = timer()\n",
    "            # print(\"minute spent after \",type, \": \",(end-start)/60)\n",
    "        print(\"Total number of entities found: \", all_count)\n",
    "    \n",
    "    main()\n",
    "\n",
    "def test(concatenated_text):\n",
    "    import json\n",
    "    from collections import defaultdict\n",
    "    from collections import Counter\n",
    "    import numpy as np\n",
    "    from ahocorapy.keywordtree import KeywordTree\n",
    "    from timeit import default_timer as timer\n",
    "    ################################################\n",
    "    # Experiment Setup\n",
    "    ################################################\n",
    "\n",
    "    entity_file = \"expt1-data/ner_data_enron_wo_pretrained.json\"\n",
    "    # Modify the variable to your test string\n",
    "    test_string = concatenated_text\n",
    "    k_value = 1  # parameter for k-eidetic search, None if looking for all entities\n",
    "\n",
    "    ################################################\n",
    "    # Create list of given entity type\n",
    "    ################################################\n",
    "    def process_entity_type(entities, type):\n",
    "        ent_list = []\n",
    "        for k, v in entities.items():\n",
    "            if k == type:\n",
    "                for s in v:\n",
    "                    entity = s.strip()\n",
    "                    if len(entity) > 3:\n",
    "                        ent_list.append(entity)\n",
    "        return ent_list\n",
    "\n",
    "    ################################################\n",
    "    # Find entities in text samples\n",
    "    ################################################\n",
    "    def find_entities(entities, text, type, k_value=None):\n",
    "        kwtree = KeywordTree(case_insensitive=False)\n",
    "        val, cnt = np.unique(entities, return_counts=True)\n",
    "\n",
    "        if k_value is not None:\n",
    "            eidetic_ents = val[cnt == 1]\n",
    "            print(\"Number of \", k_value, \" eidetic\", type, \" ents: \", eidetic_ents.size)\n",
    "            for ent in eidetic_ents:\n",
    "                kwtree.add(ent)\n",
    "        else:\n",
    "            unique_ents = set(val)\n",
    "            print(\"Number of unique\", type, \" ents: \", len(list(unique_ents)))\n",
    "            for ent in unique_ents:\n",
    "                kwtree.add(ent)\n",
    "\n",
    "        kwtree.finalize()\n",
    "        lines = text.split('\\n')  # Split the test_string into lines\n",
    "        results = kwtree.search_all(' '.join(lines))\n",
    "        result_set = set([result[0] for result in results])\n",
    "        return result_set\n",
    "    def main():\n",
    "        start = timer()\n",
    "\n",
    "        all_count = 0\n",
    "\n",
    "        all_ent = json.load(open(\"extractionfiles/\" + entity_file))\n",
    "\n",
    "        search_types = [\"PERSON\", \"ORG\", \"LOC\", \"GPE\", \"FAC\", \"MONEY\", \"CARDINAL\"]\n",
    "\n",
    "        for type in search_types:\n",
    "            select_ents = process_entity_type(all_ent, type)\n",
    "            found_ents = find_entities(select_ents, test_string, type, k_value)\n",
    "            print(type, \" count: \", len(found_ents))\n",
    "            print(found_ents)\n",
    "            all_count += len(found_ents)\n",
    "            end = timer()\n",
    "            # print(\"minute spent after \",type, \": \",(end-start)/60)\n",
    "        print(\"Total number of entities found: \", all_count)\n",
    "    \n",
    "    main()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample Common Crawl dataset: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n",
    "\n",
    "samples = []\n",
    "\n",
    "shuffled_dataset = dataset.shuffle(buffer_size=20000, seed=42)\n",
    "for i, example in enumerate(shuffled_dataset):\n",
    "    text = example[\"text\"]\n",
    "    \n",
    "    # split the text into chunks of 100 characters\n",
    "    chunks = [text[i:i+100] for i in range(0, len(text), 100)]\n",
    "    \n",
    "    # extract the first 20,000 chunks as a list of text samples\n",
    "    if len(samples) < 20000:\n",
    "        text = random.choice(chunks)\n",
    "        if len(text) == 100:\n",
    "            samples.append(text)\n",
    "    else:\n",
    "        break\n",
    "    if i % 100 == 0: \n",
    "        print(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save samples to file and load:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save samples to file: \n",
    "import csv\n",
    "with open('extractionfiles/samples.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter='|')\n",
    "    writer.writerow(['text']) # write header row\n",
    "    for text in samples:\n",
    "        writer.writerow([text])\n",
    "\n",
    "# read the samples from the csv file\n",
    "with open('extractionfiles/samples.csv', mode='r') as file:\n",
    "    reader = csv.reader(file, delimiter='|')\n",
    "    next(reader) # skip the header row\n",
    "    new_samples = [row[0] for row in reader]\n",
    "\n",
    "# new_samples\n",
    "new_samples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate using Common Crawl prompts: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key_path=\"api_key.txt\"\n",
    "\n",
    "responses = []\n",
    "\n",
    "for i, sample in enumerate(samples[:5000]):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"curie\", #REPLACE WITH YOUR FINE-TUNED MODEL ID FROM OPENAI\n",
    "        prompt=sample, \n",
    "        temperature=1,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    responses.append(response['choices'][0]['text'])\n",
    "    print(i, response['choices'][0]['text'])\n",
    "responses"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generations from blank prompt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key_path=\"api_key.txt\"\n",
    "\n",
    "responses = []\n",
    "\n",
    "for i in range(900):\n",
    "    response = openai.Completion.create(\n",
    "        model= \"curie\", #REPLACE WITH YOUR FINE-TUNED MODEL ID FROM OPENAI\n",
    "        prompt=\"\", \n",
    "        temperature=0.5,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    responses.append(response['choices'][0]['text'])\n",
    "    file_path = \"extractionfiles/responses_blank_output.txt\"  # specify the path and filename\n",
    "    with open(file_path, \"a\") as file:\n",
    "        file.write(response['choices'][0]['text'] + \"\\n\")\n",
    "    print(i)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# After you save your generations for the fine-tuned and base models, load them as strings as variables `finetuned_generations` and `basemodel_generations`, respectively. Then, run the `test_set_difference()` function below: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_set_difference(finetuned_generations, basemodel_generations)    "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.6 64-bit ('nlp': conda)"
  },
  "interpreter": {
   "hash": "e678fb984bd189314f995ebb48f7b1642aa5a73c8e1bc8f4c349e3947f412d39"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}